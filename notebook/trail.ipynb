{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENT_VALID_EXAMPLES = 0.1\n",
    "BATCHSIZE = 128\n",
    "CLASSES = 10\n",
    "EPOCHS = 10\n",
    "DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=3, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=3, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout: float, output_dims: List[int]):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        input_dim: int = 28 * 28\n",
    "        for output_dim in output_dims:\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = output_dim\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, CLASSES))\n",
    "\n",
    "        self.layers: nn.Module = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.layers(data)\n",
    "        return F.log_softmax(logits, dim=1)\n",
    "    \n",
    "Net(dropout=0.2, output_dims=[128, 64, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningNet(pl.LightningModule):\n",
    "    def __init__(self, dropout: float, output_dims: List[int]):\n",
    "        super().__init__()\n",
    "        self.model = Net(dropout, output_dims)\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(data.view(-1, 28 * 28))\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int) -> torch.Tensor:\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        return F.nll_loss(output, target)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx: int) -> None:\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        accuracy = pred.eq(target.view_as(pred)).float().mean()\n",
    "        self.log(\"val_acc\", accuracy, sync_dist=True)\n",
    "        self.log(\"hp_metric\", accuracy, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self) -> optim.Optimizer:\n",
    "        return optim.Adam(self.model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        self.mnist_test = datasets.FashionMNIST(\n",
    "            self.data_dir, train=False, download=True, transform=transforms.ToTensor()\n",
    "        )\n",
    "        mnist_full = datasets.FashionMNIST(\n",
    "            self.data_dir, train=True, download=True, transform=transforms.ToTensor()\n",
    "        )\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.mnist_train, batch_size=self.batch_size, shuffle=True, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.mnist_val, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.mnist_test, batch_size=self.batch_size, shuffle=False, pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    output_dims = [\n",
    "        trial.suggest_int(\"n_units_l{}\".format(i), 4, 128, log=True) for i in range(n_layers)\n",
    "    ]\n",
    "\n",
    "    model = LightningNet(dropout, output_dims)\n",
    "    datamodule = FashionMNISTDataModule(data_dir=DIR, batch_size=BATCHSIZE)\n",
    "    callback = PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,\n",
    "        limit_val_batches=PERCENT_VALID_EXAMPLES,\n",
    "        enable_checkpointing=False,\n",
    "        max_epochs=EPOCHS,\n",
    "        accelerator=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=2,\n",
    "        callbacks=[callback],\n",
    "        strategy=\"ddp_notebook\",\n",
    "        # strategy=\"ddp_spawn\",\n",
    "    )\n",
    "    hyperparameters = dict(n_layers=n_layers, dropout=dropout, output_dims=output_dims)\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    callback.check_pruned()\n",
    "\n",
    "    return trainer.callback_metrics[\"val_acc\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner: optuna.pruners.BasePruner = optuna.pruners.MedianPruner()\n",
    "storage = \"sqlite:///example.db\"\n",
    "study = optuna.create_study(\n",
    "    study_name=\"pl_ddp\",\n",
    "    storage=storage,\n",
    "    direction=\"maximize\",\n",
    "    pruner=pruner,\n",
    "    load_if_exists=True,\n",
    ")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import (efficientnet_b0, EfficientNet_B0_Weights,\n",
    "                                efficientnet_b1, EfficientNet_B1_Weights, \n",
    "                                efficientnet_b2, EfficientNet_B2_Weights, \n",
    "                                efficientnet_b3, EfficientNet_B3_Weights, \n",
    "                                efficientnet_b4, EfficientNet_B4_Weights, \n",
    "                                efficientnet_v2_m, EfficientNet_V2_M_Weights, \n",
    "                                efficientnet_v2_s, EfficientNet_V2_S_Weights)\n",
    "\n",
    "NUM_CLASS = 3\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, base_model, dropout: float, output_dims: List[int]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = base_model\n",
    "        input_dim: int = base_model.classifier[1].in_features\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        for output_dim in output_dims:\n",
    "            layers.append(nn.Linear(input_dim, output_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = output_dim\n",
    "        layers.append(nn.Linear(input_dim, NUM_CLASS))\n",
    "\n",
    "        self.base_model.classifier = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "Network(base_model=efficientnet_b4(), dropout=0.3, output_dims=[128, 64, 32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningNet(pl.LightningModule):\n",
    "    def __init__(self, base_model, dropout: float, output_dims: List[int]) -> None:\n",
    "        super().__init__()\n",
    "        self.model = Network(base_model, dropout, output_dims)\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(data)\n",
    "\n",
    "    def training_step(self, batch: List[torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        return F.nll_loss(output, target)\n",
    "\n",
    "    def validation_step(self, batch: List[torch.Tensor], batch_idx: int) -> None:\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        accuracy = pred.eq(target.view_as(pred)).float().mean()\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "        self.log(\"hp_metric\", accuracy, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self) -> optim.Optimizer:\n",
    "        return optim.Adam(self.model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, input_shape, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, 'img_path']\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        xmin, ymin, xmax, ymax = self.df.loc[idx, ['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "\n",
    "        img = img[ymin:ymax, xmin:xmax]\n",
    "        img = cv2.resize(img, self.input_shape[:-1], interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = int(self.df.loc[idx, 'sparse_label'])\n",
    "        return torch.from_numpy(img.transpose((2, 0, 1))), torch.tensor(label)\n",
    "\n",
    "# Example of using torchvision transforms for data augmentation\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from glob import glob \n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from src.config import *\n",
    "from src.dataset import DataModule, ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_name</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>sparse_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>papular</td>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>1635</td>\n",
       "      <td>2918</td>\n",
       "      <td>00241a_A_P.jpg</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>3.0</td>\n",
       "      <td>D:/Projects/Skin Disease Detection/Dataset/Lat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scabies</td>\n",
       "      <td>1006</td>\n",
       "      <td>1260</td>\n",
       "      <td>1713</td>\n",
       "      <td>2143</td>\n",
       "      <td>00240b_A_S.jpg</td>\n",
       "      <td>1960</td>\n",
       "      <td>4032</td>\n",
       "      <td>6.0</td>\n",
       "      <td>D:/Projects/Skin Disease Detection/Dataset/Lat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atopic</td>\n",
       "      <td>215</td>\n",
       "      <td>424</td>\n",
       "      <td>457</td>\n",
       "      <td>706</td>\n",
       "      <td>0052b_A_A.jpg</td>\n",
       "      <td>720</td>\n",
       "      <td>1280</td>\n",
       "      <td>9.0</td>\n",
       "      <td>D:/Projects/Skin Disease Detection/Dataset/Lat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scabies</td>\n",
       "      <td>138</td>\n",
       "      <td>94</td>\n",
       "      <td>633</td>\n",
       "      <td>643</td>\n",
       "      <td>0044a_A_S.jpg</td>\n",
       "      <td>1280</td>\n",
       "      <td>1280</td>\n",
       "      <td>14.0</td>\n",
       "      <td>D:/Projects/Skin Disease Detection/Dataset/Lat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scabies</td>\n",
       "      <td>354</td>\n",
       "      <td>866</td>\n",
       "      <td>950</td>\n",
       "      <td>1217</td>\n",
       "      <td>0044a_A_S.jpg</td>\n",
       "      <td>1280</td>\n",
       "      <td>1280</td>\n",
       "      <td>14.0</td>\n",
       "      <td>D:/Projects/Skin Disease Detection/Dataset/Lat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label_name  xmin  ymin  xmax  ymax      image_name  image_width  \\\n",
       "0    papular     0   919  1635  2918  00241a_A_P.jpg         1960   \n",
       "1    scabies  1006  1260  1713  2143  00240b_A_S.jpg         1960   \n",
       "2     atopic   215   424   457   706   0052b_A_A.jpg          720   \n",
       "3    scabies   138    94   633   643   0044a_A_S.jpg         1280   \n",
       "4    scabies   354   866   950  1217   0044a_A_S.jpg         1280   \n",
       "\n",
       "   image_height  patient_id  \\\n",
       "0          4032         3.0   \n",
       "1          4032         6.0   \n",
       "2          1280         9.0   \n",
       "3          1280        14.0   \n",
       "4          1280        14.0   \n",
       "\n",
       "                                            img_path  sparse_label  \n",
       "0  D:/Projects/Skin Disease Detection/Dataset/Lat...             1  \n",
       "1  D:/Projects/Skin Disease Detection/Dataset/Lat...             2  \n",
       "2  D:/Projects/Skin Disease Detection/Dataset/Lat...             0  \n",
       "3  D:/Projects/Skin Disease Detection/Dataset/Lat...             2  \n",
       "4  D:/Projects/Skin Disease Detection/Dataset/Lat...             2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_paths = glob(f'{DATA_DIR}/images/*')\n",
    "df = pd.read_csv(f'{DATA_DIR}/verified_annotation_from_xml.csv')\n",
    "\n",
    "df = pd.read_csv(f'{DATA_DIR}/verified_annotation_from_xml.csv')\n",
    "df['img_path'] =f'{DATA_DIR}/images/' + df['image_name']\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df['label_name'] = df['label_name'].apply(lambda x: x.lower())\n",
    "df['sparse_label'] = df['label_name'].map({'atopic': 0, 'papular': 1,'scabies': 2})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, input_shape, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.loc[idx, 'img_path']\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        xmin, ymin, xmax, ymax = self.df.loc[idx, ['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "\n",
    "        img = img[ymin:ymax, xmin:xmax]\n",
    "        img = cv2.resize(img, self.input_shape[:-1], interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        sparse_label = int(self.df.loc[idx, 'sparse_label'])\n",
    "        sparse_label_tensor = torch.tensor(sparse_label)\n",
    "\n",
    "        cat_label = F.one_hot(sparse_label_tensor, num_classes=NUM_CLASSES)\n",
    "        return torch.from_numpy(img.transpose((2, 0, 1))), cat_label\n",
    "    \n",
    "\n",
    "NUM_CLASS = 3\n",
    "BATCH_SIZE = 3\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, model_name: str) -> None:\n",
    "        super().__init__()\n",
    "        self.resize_size = RESIZE_SIZE[model_name][:-1]\n",
    "        self.crop_size = CROP_SIZE[model_name][:-1]\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.num_workers = NUM_WORKERS\n",
    "        self.prepare_data()\n",
    "        self.setup() \n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        self.df = pd.read_csv(f'{DATA_DIR}/verified_annotation_from_xml.csv')\n",
    "        self.df['img_path'] =f'{DATA_DIR}/images/' + self.df['image_name']\n",
    "        self.df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "        self.df['label_name'] = self.df['label_name'].apply(lambda x: x.lower())\n",
    "        self.df['sparse_label'] = self.df['label_name'].map({'atopic': 0, 'papular': 1,'scabies': 2})\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "\n",
    "        gs = GroupShuffleSplit(n_splits=2, train_size=.85, random_state=42)\n",
    "\n",
    "        train_val_idx, test_idx = next(gs.split(self.df,groups=self.df.patient_id))\n",
    "        train_val_df = self.df.iloc[train_val_idx]\n",
    "        test_df = self.df.iloc[test_idx]\n",
    "\n",
    "        train_idx, val_idx = next(gs.split(train_val_df, groups=train_val_df.patient_id))\n",
    "        train_df = train_val_df.iloc[train_idx]\n",
    "        val_df = train_val_df.iloc[val_idx]\n",
    "\n",
    "        train_df.reset_index(drop=True, inplace=True)\n",
    "        val_df.reset_index(drop=True, inplace=True)\n",
    "        test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        self.train_ds = ImageDataset(\n",
    "            df=train_df, \n",
    "            input_shape=self.resize_size, \n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToPILImage(),\n",
    "                    # transforms.Resize(self.resize_size),\n",
    "                    transforms.CenterCrop(self.crop_size),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.val_ds = ImageDataset(\n",
    "            df=val_df, \n",
    "            input_shape=self.resize_size, \n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToPILImage(),\n",
    "                    # transforms.Resize(self.resize_size),\n",
    "                    transforms.CenterCrop(self.crop_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.test_ds = ImageDataset(\n",
    "            df=test_df, \n",
    "            input_shape=self.resize_size, \n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToPILImage(),\n",
    "                    # transforms.Resize(self.resize_size),\n",
    "                    transforms.CenterCrop(self.crop_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(model_name='efficientnet_b0')\n",
    "for x, y in data_module.train_dataloader():\n",
    "    print(x.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
